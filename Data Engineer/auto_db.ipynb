{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AUTOMATIZACIÓN DE INGESTA DE DATOS NUEVOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar la librería WATCHDOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watchdog in c:\\python312\\lib\\site-packages (4.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\python312\\lib\\site-packages (2.2.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usuario\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usuario\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: SQLAlchemy in c:\\python312\\lib\\site-packages (2.0.30)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\python312\\lib\\site-packages (from SQLAlchemy) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\python312\\lib\\site-packages (from SQLAlchemy) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyodbc in c:\\python312\\lib\\site-packages (5.1.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install watchdog\n",
    "%pip install pandas \n",
    "%pip install numpy \n",
    "%pip install SQLAlchemy \n",
    "%pip install pyodbc\n",
    "%pip install nbformat\n",
    "\n",
    "import pyodbc\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:228: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  game_stats['ID_game_stats'] = range(1, len(game_stats) + 1)  # Creación del id\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:299: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_season = pd.read_sql_query(f\"SELECT * FROM season\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:304: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_State = pd.read_sql_query(f\"SELECT * FROM State\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:309: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_City = pd.read_sql_query(f\"SELECT * FROM City\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:314: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_Arena = pd.read_sql_query(f\"SELECT * FROM Arena\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:319: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_Location = pd.read_sql_query(f\"SELECT * FROM Location\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:324: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_Proveniencia = pd.read_sql_query(f\"SELECT * FROM Proveniencia\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:329: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_teams = pd.read_sql_query(f\"SELECT * FROM teams\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:334: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_teams_History = pd.read_sql_query(f\"SELECT * FROM teams_History\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:339: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_Games = pd.read_sql_query(f\"SELECT * FROM Games\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:344: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_Game_stats = pd.read_sql_query(f\"SELECT * FROM Game_stats\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:349: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_Player = pd.read_sql_query(f\"SELECT * FROM Player\", conn)\n",
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_23940\\3755356686.py:354: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_viejo_Draft = pd.read_sql_query(f\"SELECT * FROM Draft\", conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo modificado: C:/Users/Usuario/Desktop/data/repo/Proyecto-Final-Henry/Data Engineer/csv\\team.csv\n"
     ]
    }
   ],
   "source": [
    "class Watcher:\n",
    "    # Directorio que se va a observar\n",
    "    # Es necesario colocar la ruta del directorio local donde esén los archivos .csv quién ejecute el código\n",
    "    DIRECTORY_TO_WATCH = \"C:/Users/Usuario/Desktop/data/repo/Proyecto-Final-Henry/Data Engineer/csv\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Inicialización del observador\n",
    "        self.observer = Observer()\n",
    "\n",
    "    def run(self):\n",
    "        # Creación de un manejador de eventos\n",
    "        event_handler = Handler()\n",
    "        # Configuración del observador para que observe el directorio y subdirectorios\n",
    "        self.observer.schedule(event_handler, self.DIRECTORY_TO_WATCH, recursive=True)\n",
    "        # Inicio del observador\n",
    "        self.observer.start()\n",
    "        try:\n",
    "            # Bucle infinito para mantener el programa en ejecución\n",
    "            while True:\n",
    "                # Pausa de 1 segundo para evitar un uso excesivo de la CPU\n",
    "                time.sleep(1)\n",
    "        except KeyboardInterrupt:\n",
    "            # Detener el observador si se detecta una interrupción del teclado (Ctrl + C)\n",
    "            self.observer.stop()\n",
    "        # Esperar a que el observador termine antes de salir del programa\n",
    "        self.observer.join()\n",
    "\n",
    "class Handler(FileSystemEventHandler):\n",
    "    @staticmethod\n",
    "    def on_modified(event):\n",
    "        # Verificar si el evento corresponde a un directorio\n",
    "        if event.is_directory:\n",
    "            return None\n",
    "        # Verificar si el evento es una modificación de archivo\n",
    "        elif event.event_type == 'modified':\n",
    "            # Aquí puedes poner el código que quieres ejecutar:\n",
    "            \n",
    "            #-------------------------------------- CÓDIGO A EJECUTAR --------------------------------------#         \n",
    "            \n",
    "            # Cargar los CSV a los dfs con las columnas que vamos a ocupar\n",
    "            df_team = pd.read_csv('csv/team.csv', usecols=['id', 'full_name','abbreviation','city', 'state'])\n",
    "            df_team_details = pd.read_csv('csv/team_details.csv', usecols=['team_id','owner', 'generalmanager','headcoach', 'arena', 'arenacapacity'])\n",
    "            df_team_history = pd.read_csv('csv/team_history.csv', usecols=['team_id', 'year_founded', 'year_active_till'])\n",
    "\n",
    "            new_teams_data = {\n",
    "                'team_id': [1610612738, 1610612739, 1610612752, 1610612740, 1610612753],\n",
    "                'owner': ['Wyc Grousbeck, Steve Pagliuca, y otros socios', 'Dan Gilbert', 'James Dolan (Madison Square Garden Sports)', 'Gayle Benson', 'RDV Sports, Inc. (familia DeVos)'],\n",
    "                'generalmanager': ['Brad Stevens', 'Mike Gansey', 'Scott Perry', 'Trajan Langdon', 'Anthony Parker'],\n",
    "                'headcoach': ['Joe Mazzulla', 'J.B. Bickerstaff', 'Tom Thibodeau', 'Willie Green', 'Jamahl Mosley'],\n",
    "                'arena': ['TD Garden', 'Rocket Mortgage FieldHouse', 'Madison Square Garden', 'Smoothie King Center', 'Amway Center'],\n",
    "                'arenacapacity': [19156, 19432, 19812, 16867, 18846]\n",
    "            }\n",
    "\n",
    "            # Crear DataFrame con nuevos datos\n",
    "            df_new_teams = pd.DataFrame(new_teams_data)\n",
    "\n",
    "            # Concatenar DataFrames\n",
    "            df_team_details = pd.concat([df_team_details, df_new_teams], ignore_index=True)\n",
    "\n",
    "            # Renombrar la columna id a team_id\n",
    "            df_team = df_team.rename(columns={'id': 'team_id'})\n",
    "\n",
    "            # Fusionar las tablas df_team y df_team_details\n",
    "            df_fusionado = pd.merge(df_team, df_team_details, on='team_id', how='outer')\n",
    "\n",
    "            # Fusionar la tabla resultante anterior con df_team_history\n",
    "            df_fusionado2 = pd.merge(df_fusionado, df_team_history, on='team_id', how='outer')\n",
    "\n",
    "            teams = df_fusionado2.rename(columns={'team_id': 'id'})\n",
    "\n",
    "            # Encontrar el valor máximo de la columna 'year_active_till'\n",
    "            max_year_active_till = teams['year_active_till'].max()\n",
    "\n",
    "            # Agregar la columna 'estado' basada en la condición\n",
    "            teams['estado'] = np.where(teams['year_active_till'] < max_year_active_till, 0, 1)\n",
    "\n",
    "            # Agregado de la tabla location al dataframe\n",
    "            # Crear un nuevo DataFrame con las columnas \"state\" y \"city\"\n",
    "            location = pd.DataFrame(teams, columns=['state','city'])\n",
    "\n",
    "            # Eliminar filas duplicadas basadas en la columna \"city\"\n",
    "            df_location_sin_duplicados = location.drop_duplicates(subset=['city'])\n",
    "\n",
    "            # Crear un nuevo DataFrame con las columnas\n",
    "            location = df_location_sin_duplicados\n",
    "\n",
    "            # Generar una columna \"ID\" con valores únicos\n",
    "            location['ID_location'] = range(1, len(location) + 1) \n",
    "\n",
    "            # Unir los DataFrames en función de las columnas \"city\" y \"state\"\n",
    "            merge_location = pd.merge(teams, location, on=['city','state'], how='left')\n",
    "\n",
    "            teams = merge_location\n",
    "\n",
    "            teams = teams.drop(columns=['city','state'])\n",
    "\n",
    "            ##Agregado de la tabla arena al dataframe\n",
    "            # Crear un nuevo DataFrame con las columnas \"arena\" y \"arenacapacity\"\n",
    "            arena = pd.DataFrame(teams, columns=['arena','arenacapacity'])\n",
    "\n",
    "            # Eliminar filas duplicadas basadas en la columna \"city\"\n",
    "            df_arena_sin_duplicados = arena.drop_duplicates(subset=['arena'])\n",
    "\n",
    "            # Crear un nuevo DataFrame con las columnas\n",
    "            arena = df_arena_sin_duplicados\n",
    "\n",
    "            # Generar una columna \"ID\" con valores únicos\n",
    "            arena['ID_arena'] = range(1, len(arena) + 1) \n",
    "\n",
    "            # Unir los DataFrames en función de las columnas \"city\" y \"state\"\n",
    "            merge_arena = pd.merge(teams, arena, on=['arenacapacity','arena'], how='left')\n",
    "\n",
    "            teams = merge_arena\n",
    "\n",
    "            teams.rename(columns={\"id\": \"ID_team\"}, inplace=True)\n",
    "\n",
    "            teams = teams.drop(columns=['arenacapacity','arena','year_founded','year_active_till','estado'])\n",
    "\n",
    "            teams_History = df_team_history.rename(columns={'team_id': 'ID_team'})\n",
    "\n",
    "            # Generar una columna \"ID\" con valores únicos\n",
    "            teams_History['ID_history'] = range(1, len(teams_History) + 1)\n",
    "\n",
    "            teams = teams.drop_duplicates(subset='ID_team')\n",
    "\n",
    "            teams.fillna('Darko Rajaković', inplace=True)\n",
    "            \n",
    "            # Df city\n",
    "            city_unique = df_team['city'].unique()  # Obtiene los valores únicos de la columna 'city'\n",
    "            city = pd.DataFrame(city_unique, columns=['city'])  # Crea un DataFrame con los valores únicos\n",
    "\n",
    "            city['ID_city'] = range(1, len(city) + 1)  #Creo el indice\n",
    "\n",
    "            # Df state\n",
    "            state_unique = df_team['state'].unique()  # Obtiene los valores únicos de la columna 'state'\n",
    "            state = pd.DataFrame(state_unique, columns=['state'])  # Crea un DataFrame con los valores únicos\n",
    "\n",
    "            state['ID_state'] = range(1, len(state) + 1)  #Creo el indice\n",
    "\n",
    "            #Megere\n",
    "            # Unir los DataFrames en función de las columnas \"city\" y \"state\"\n",
    "            merge_city = pd.merge(city, location, on=['city'], how='left')\n",
    "            merge_total = pd.merge(state, merge_city, on=['state'], how='left')\n",
    "\n",
    "            merge_total = merge_total.drop(columns=['state','city'])\n",
    "\n",
    "            #Borro columnas de location\n",
    "            location = merge_total\n",
    "            \n",
    "            # Leer los archivos CSV\n",
    "            df_draft_combine_stats = pd.read_csv('csv/draft_combine_stats.csv')\n",
    "            df_draft_history = pd.read_csv('csv/draft_history.csv')\n",
    "\n",
    "            # Seleccion de las columnas a utilizar\n",
    "            columnas_draft_combine_stats  = ['player_id','weight','wingspan','standing_reach','body_fat_pct','standing_vertical_leap','max_vertical_leap','lane_agility_time','modified_lane_agility_time','three_quarter_sprint','bench_press']\n",
    "            df_draft_combine_stats_reducido = df_draft_combine_stats.loc[:,columnas_draft_combine_stats]\n",
    "\n",
    "            columnas_draft_history  = ['person_id','season','player_name','round_number','round_pick','overall_pick','team_id','organization','organization_type']\n",
    "            df_draft_history_reducido = df_draft_history.loc[:,columnas_draft_history]\n",
    "\n",
    "            df_draft_history_reducido.rename(columns={\"person_id\": \"player_id\"}, inplace=True)\n",
    "\n",
    "            # DataFrame Draft\n",
    "            draft = pd.merge(df_draft_combine_stats_reducido, df_draft_history_reducido, on='player_id', how='outer')\n",
    "\n",
    "            # Df proveniencia\n",
    "            # Crear un nuevo DataFrame con las columnas \"organization\" y \"organization_type\"\n",
    "            proveniencia = pd.DataFrame(draft, columns=['organization','organization_type'])  \n",
    "\n",
    "            # Eliminar filas duplicadas basadas en la columna \"organization\"\n",
    "            df_sin_duplicados = proveniencia.drop_duplicates(subset=[\"organization\"])\n",
    "\n",
    "            # Crear un nuevo DataFrame con las columnas \n",
    "            proveniencia = df_sin_duplicados\n",
    "\n",
    "            # Generar una columna \"ID\" con valores únicos\n",
    "            proveniencia[\"ID_proveniencia\"] = range(1, len(proveniencia) + 1)\n",
    "\n",
    "            # Unir los DataFrames en función de las columnas \"organization\" y \"organization_type\"\n",
    "            merge = pd.merge(draft, proveniencia, on=[\"organization\", \"organization_type\"], how=\"left\")\n",
    "\n",
    "            draft = merge\n",
    "\n",
    "            draft = draft.drop(columns=['organization','organization_type'])\n",
    "\n",
    "            draft = draft.rename(columns={'player_id':'ID_player', 'team_id':'ID_team'})\n",
    "\n",
    "            draft = draft[draft['ID_team']>=1610612737]\n",
    "            \n",
    "            # Leer el archivo CSV\n",
    "            df_common_player_info = pd.read_csv('csv/common_player_info.csv')\n",
    "            df_inactive_players = pd.read_csv('csv/inactive_players.csv')\n",
    "            df_player = pd.read_csv('csv/player.csv')\n",
    "\n",
    "            #DF Players\n",
    "            player = df_common_player_info.drop(columns=['display_first_last','school','country','display_last_comma_first','display_fi_last','player_slug','last_affiliation','rosterstatus','games_played_current_season_flag','team_name','team_abbreviation','team_code','team_city','playercode','from_year','to_year','dleague_flag','nba_flag','games_played_flag','draft_year','draft_round','draft_number','greatest_75_flag'])\n",
    "            # Renombrar la columna id a person_id\n",
    "            df_player = df_player.rename(columns={'id': 'person_id'})\n",
    "\n",
    "            # Fusionar las tablas df_player y player\n",
    "            player = pd.merge(player, df_player, on='person_id', how='inner')\n",
    "            player = player.drop(columns=['last_name_y','first_name_y','full_name'])\n",
    "\n",
    "            # Renombrar la columna id a ID_player\n",
    "            player = player.rename(columns={'person_id': 'ID_player', 'first_name_x':'first_name', 'last_name_x':'last_name', 'team_id':'ID_team'})\n",
    "\n",
    "            player = player[player['ID_team']>=1610612737]\n",
    "            \n",
    "            # Cargar los archivos CSV\n",
    "            df_game = pd.read_csv('csv/game.csv')  \n",
    "            df_game_info = pd.read_csv('csv/game_info.csv')  \n",
    "            df_game_summary = pd.read_csv('csv/game_summary.csv')  \n",
    "            df_line_score = pd.read_csv('csv/line_score.csv')\n",
    "\n",
    "            # Unir las tablas\n",
    "            df_combine = df_game.merge(df_game_info, on='game_id', how='inner')\n",
    "\n",
    "            df_combine_summary = df_combine.merge(df_game_summary, on='game_id', how='inner')\n",
    "\n",
    "            df_combine_total = df_combine_summary.merge(df_line_score, on='game_id', how='inner')\n",
    "\n",
    "            # Seleccionar las columnas\n",
    "\n",
    "            game_stats = df_combine_total[['game_id','pts_home_x', 'pts_qtr1_home', 'pts_qtr2_home', 'pts_qtr3_home', 'pts_qtr4_home', \n",
    "                                            'pts_qtr1_away', 'pts_qtr2_away', 'pts_qtr3_away', 'pts_qtr4_away', 'pts_away_x', \n",
    "                                            'attendance', 'game_time', 'natl_tv_broadcaster_abbreviation', 'live_period_time_bcast']]\n",
    "\n",
    "            game_stats['ID_game_stats'] = range(1, len(game_stats) + 1)  # Creación del id\n",
    "\n",
    "\n",
    "            df_games = df_combine_total[['game_id', 'game_date_x', 'team_id_home_x', 'wl_home', 'team_id_away_x', 'wl_away', 'season_id', 'season', 'season_type']]\n",
    "\n",
    "            # Renombrar la columna team_id_home a ID_team en df_games\n",
    "            df_games = df_games.rename(columns={'team_id_home_x': 'ID_team'})\n",
    "\n",
    "            games = pd.merge(df_games, teams[['ID_team', 'ID_location']], on='ID_team', how='outer')\n",
    "\n",
    "\n",
    "            # Renombrar la columna game_id a ID_game\n",
    "            games = games.rename(columns={'game_id': 'ID_game', 'game_date_x': 'game_date', 'team_id_away_x': 'team_id_away'})\n",
    "\n",
    "\n",
    "            # Creación de tabla de Season \n",
    "            df_season = df_combine_total[['season_id', 'season', 'season_type']]\n",
    "            season = df_season.drop_duplicates(subset=['season_id'])\n",
    "            season = season.rename(columns={'season_id': 'ID_season'})\n",
    "            season = season.drop_duplicates(subset='ID_season')\n",
    "\n",
    "            games = games.rename(columns={'season_id': 'ID_season'})\n",
    "            games = games.drop(columns=['season','season_type'])\n",
    "            games = games.drop_duplicates(subset='ID_game')\n",
    "\n",
    "            game_stats = game_stats.rename(columns={'game_id': 'ID_game'})\n",
    "\n",
    "            # ID_season en tabla de Draft\n",
    "            draft = pd.merge(draft, season[['ID_season', 'season']], on='season', how='inner')\n",
    "            draft_d = draft.drop(columns=['season'])\n",
    "            draft = draft_d\n",
    "\n",
    "            draft['ID_draft'] = range(1, len(draft) + 1)  # Creación del id\n",
    "\n",
    "            games = games[games['ID_team']>=1610612737]\n",
    "\n",
    "\n",
    "            # Arreglo de errores\n",
    "\n",
    "            prueba = pd.merge(player,draft[['ID_player','player_name']],on='ID_player',how='outer')\n",
    "            prueba = prueba.drop_duplicates('ID_player')\n",
    "            prueba_2 = prueba.dropna(subset=['player_name'])\n",
    "            player = prueba_2.drop(columns=['first_name','last_name'])\n",
    "\n",
    "            game_stats = game_stats.drop_duplicates(subset='ID_game')\n",
    "\n",
    "            game_stats = pd.merge(game_stats, games['ID_game'], on='ID_game', how='inner')\n",
    "            games = games.dropna(subset=['ID_game'])\n",
    "            \n",
    "            from sqlalchemy import create_engine\n",
    "            import pyodbc\n",
    "\n",
    "            # Parámetros de la conexión\n",
    "            server = 'localhost\\\\SQLEXPRESS'  # Reemplaza con el nombre de tu servidor\n",
    "            database = 'DAFT01_Grupo1_nba'  # Reemplaza con el nombre de tu base de datos\n",
    "\n",
    "            # Crear la cadena de conexión\n",
    "            conn_str = (\n",
    "                'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "                'SERVER=' + server + ';'  # Reemplaza con el nombre de tu servidor\n",
    "                'DATABASE=' + database + ';'  # La base de datos a la que te quieres conectar\n",
    "                'Trusted_Connection=yes;'\n",
    "            )\n",
    "\n",
    "            # Crear la conexión\n",
    "            conn = pyodbc.connect(conn_str, autocommit=True)  # El autocommit en True permite crear la base desde el código\n",
    "\n",
    "            # Crear un cursor\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Season\n",
    "            df_viejo_season = pd.read_sql_query(f\"SELECT * FROM season\", conn)\n",
    "            merged_season = season.merge(df_viejo_season, on='ID_season', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            season_nuevo = merged_season[merged_season['_merge'] == 'left_only'].drop(columns=[col for col in merged_season.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # State\n",
    "            df_viejo_State = pd.read_sql_query(f\"SELECT * FROM State\", conn)\n",
    "            merged_State = state.merge(df_viejo_State, on='ID_state', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            State_nuevo = merged_State[merged_State['_merge'] == 'left_only'].drop(columns=[col for col in merged_State.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # City\n",
    "            df_viejo_City = pd.read_sql_query(f\"SELECT * FROM City\", conn)\n",
    "            merged_City = city.merge(df_viejo_City, on='ID_city', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            City_nuevo = merged_City[merged_City['_merge'] == 'left_only'].drop(columns=[col for col in merged_City.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # Arena\n",
    "            df_viejo_Arena = pd.read_sql_query(f\"SELECT * FROM Arena\", conn)\n",
    "            merged_Arena = arena.merge(df_viejo_Arena, on='ID_arena', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            Arena_nuevo = merged_Arena[merged_Arena['_merge'] == 'left_only'].drop(columns=[col for col in merged_Arena.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # Location\n",
    "            df_viejo_Location = pd.read_sql_query(f\"SELECT * FROM Location\", conn)\n",
    "            merged_Location = location.merge(df_viejo_Location, on='ID_location', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            Location_nuevo = merged_Location[merged_Location['_merge'] == 'left_only'].drop(columns=[col for col in merged_Location.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # Proveniencia\n",
    "            df_viejo_Proveniencia = pd.read_sql_query(f\"SELECT * FROM Proveniencia\", conn)\n",
    "            merged_Proveniencia = proveniencia.merge(df_viejo_Proveniencia, on='ID_proveniencia', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            Proveniencia_nuevo = merged_Proveniencia[merged_Proveniencia['_merge'] == 'left_only'].drop(columns=[col for col in merged_Proveniencia.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # teams\n",
    "            df_viejo_teams = pd.read_sql_query(f\"SELECT * FROM teams\", conn)\n",
    "            merged_teams = teams.merge(df_viejo_teams, on='ID_team', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            teams_nuevo = merged_teams[merged_teams['_merge'] == 'left_only'].drop(columns=[col for col in merged_teams.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # teams_History\n",
    "            df_viejo_teams_History = pd.read_sql_query(f\"SELECT * FROM teams_History\", conn)\n",
    "            merged_teams_History = teams_History.merge(df_viejo_teams_History, on='ID_history', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            teams_History_nuevo = merged_teams_History[merged_teams_History['_merge'] == 'left_only'].drop(columns=[col for col in merged_teams_History.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # Games\n",
    "            df_viejo_Games = pd.read_sql_query(f\"SELECT * FROM Games\", conn)\n",
    "            merged_Games = games.merge(df_viejo_Games, on='ID_game', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            Games_nuevo = merged_Games[merged_Games['_merge'] == 'left_only'].drop(columns=[col for col in merged_Games.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # Game_stats\n",
    "            df_viejo_Game_stats = pd.read_sql_query(f\"SELECT * FROM Game_stats\", conn)\n",
    "            merged_Game_stats = game_stats.merge(df_viejo_Game_stats, on='ID_game_stats', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            Game_stats_nuevo = merged_Game_stats[merged_Game_stats['_merge'] == 'left_only'].drop(columns=[col for col in merged_Game_stats.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # Player\n",
    "            df_viejo_Player = pd.read_sql_query(f\"SELECT * FROM Player\", conn)\n",
    "            merged_Player = player.merge(df_viejo_Player, on='ID_player', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            Player_nuevo = merged_Player[merged_Player['_merge'] == 'left_only'].drop(columns=[col for col in merged_Player.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # Draft\n",
    "            df_viejo_Draft = pd.read_sql_query(f\"SELECT * FROM Draft\", conn)\n",
    "            merged_Draft = draft.merge(df_viejo_Draft, on='ID_draft', how='left', suffixes=('', '_old'), indicator=True)\n",
    "            Draft_nuevo = merged_Draft[merged_Draft['_merge'] == 'left_only'].drop(columns=[col for col in merged_Draft.columns if col.endswith('_old')] + ['_merge'])\n",
    "\n",
    "            # Close the cursor and the connection\n",
    "            cursor.close()\n",
    "            conn.close()\n",
    "\n",
    "            # Parámetros de la conexión\n",
    "            server = 'localhost\\\\SQLEXPRESS'  # Reemplaza con el nombre de tu servidor\n",
    "            database = 'DAFT01_Grupo1_nba'  # Reemplaza con el nombre de tu base de datos\n",
    "\n",
    "            # Crear la cadena de conexión utilizando la autenticación de Windows\n",
    "            connection_string = f'mssql+pyodbc://@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server&trusted_connection=yes'\n",
    "\n",
    "            # Crear el motor de conexión\n",
    "            engine = create_engine(connection_string)\n",
    "\n",
    "            season_nuevo.to_sql('season', engine, if_exists='append', index=False)\n",
    "            State_nuevo.to_sql('State', engine, if_exists='append', index=False)\n",
    "            City_nuevo.to_sql('City', engine, if_exists='append', index=False)\n",
    "            Arena_nuevo.to_sql('Arena', engine, if_exists='append', index=False)\n",
    "            Location_nuevo.to_sql('Location', engine, if_exists='append', index=False)\n",
    "            Proveniencia_nuevo.to_sql('Proveniencia', engine, if_exists='append', index=False)\n",
    "            teams_nuevo.to_sql('teams', engine, if_exists='append', index=False)\n",
    "            teams_History_nuevo.to_sql('teams_History', engine, if_exists='append', index=False)\n",
    "            Games_nuevo.to_sql('Games', engine, if_exists='append', index=False)\n",
    "            Game_stats_nuevo.to_sql('Game_stats', engine, if_exists='append', index=False)\n",
    "            Player_nuevo.to_sql('Player', engine, if_exists='append', index=False)\n",
    "            Draft_nuevo.to_sql('Draft', engine, if_exists='append', index=False)\n",
    "\n",
    "            #-------------------------------------- FIN DEL CÓDIGO --------------------------------------# \n",
    "\n",
    "            print(f'Archivo modificado: {event.src_path}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Crear una instancia de la clase Watcher y ejecutar el método run()\n",
    "    w = Watcher()\n",
    "    w.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
